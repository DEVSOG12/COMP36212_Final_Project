\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Part I - Stochastic Gradient Descent}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Implementation of Batch SGD}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Gradient Verification}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}SGD Training Results}{2}{subsection.1.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sgd_accuracy}{{1a}{2}{SGD accuracy over iterations with batch size $m=10$ and learning rate $\eta =0.1$}{figure.caption.1}{}}
\newlabel{sub@fig:sgd_accuracy}{{a}{2}{SGD accuracy over iterations with batch size $m=10$ and learning rate $\eta =0.1$}{figure.caption.1}{}}
\newlabel{fig:sgd_loss}{{1b}{2}{SGD loss over iterations with batch size $m=10$ and learning rate $\eta =0.1$}{figure.caption.1}{}}
\newlabel{sub@fig:sgd_loss}{{b}{2}{SGD loss over iterations with batch size $m=10$ and learning rate $\eta =0.1$}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}SGD Performance Analysis}{2}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Part II - Improving Convergence}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Effect of Batch Size and Learning Rate}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Batch Size Analysis}{2}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{fig:batch_size_accuracy}{{2a}{2}{Effect of batch size on accuracy with learning rate $\eta =0.1$}{figure.caption.2}{}}
\newlabel{sub@fig:batch_size_accuracy}{{a}{2}{Effect of batch size on accuracy with learning rate $\eta =0.1$}{figure.caption.2}{}}
\newlabel{fig:batch_size_loss}{{2b}{2}{Effect of batch size on loss with learning rate $\eta =0.1$}{figure.caption.2}{}}
\newlabel{sub@fig:batch_size_loss}{{b}{2}{Effect of batch size on loss with learning rate $\eta =0.1$}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Learning Rate Analysis}{3}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{fig:learning_rate_accuracy}{{3a}{3}{Effect of learning rate on accuracy with batch size $m=10$}{figure.caption.3}{}}
\newlabel{sub@fig:learning_rate_accuracy}{{a}{3}{Effect of learning rate on accuracy with batch size $m=10$}{figure.caption.3}{}}
\newlabel{fig:learning_rate_loss}{{3b}{3}{Effect of learning rate on loss with batch size $m=10$}{figure.caption.3}{}}
\newlabel{sub@fig:learning_rate_loss}{{b}{3}{Effect of learning rate on loss with batch size $m=10$}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Learning Rate Decay Implementation}{4}{subsection.2.2}\protected@file@percent }
\newlabel{fig:lr_decay_accuracy}{{4a}{4}{Effect of learning rate decay on accuracy}{figure.caption.4}{}}
\newlabel{sub@fig:lr_decay_accuracy}{{a}{4}{Effect of learning rate decay on accuracy}{figure.caption.4}{}}
\newlabel{fig:lr_decay_loss}{{4b}{4}{Effect of learning rate decay on loss}{figure.caption.4}{}}
\newlabel{sub@fig:lr_decay_loss}{{b}{4}{Effect of learning rate decay on loss}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Momentum Implementation}{4}{subsection.2.3}\protected@file@percent }
\newlabel{fig:momentum_accuracy}{{5a}{4}{Effect of momentum on accuracy}{figure.caption.5}{}}
\newlabel{sub@fig:momentum_accuracy}{{a}{4}{Effect of momentum on accuracy}{figure.caption.5}{}}
\newlabel{fig:momentum_loss}{{5b}{4}{Effect of momentum on loss}{figure.caption.5}{}}
\newlabel{sub@fig:momentum_loss}{{b}{4}{Effect of momentum on loss}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Combined Learning Rate Decay and Momentum}{5}{subsection.2.4}\protected@file@percent }
\newlabel{fig:combined_accuracy}{{6a}{5}{Effect of combined approaches on accuracy}{figure.caption.6}{}}
\newlabel{sub@fig:combined_accuracy}{{a}{5}{Effect of combined approaches on accuracy}{figure.caption.6}{}}
\newlabel{fig:combined_loss}{{6b}{5}{Effect of combined approaches on loss}{figure.caption.6}{}}
\newlabel{sub@fig:combined_loss}{{b}{5}{Effect of combined approaches on loss}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Part III - Adaptive Learning}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Adam Optimizer}{5}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Adam Performance Analysis}{6}{subsection.3.2}\protected@file@percent }
\newlabel{fig:adam_accuracy}{{7a}{6}{Adam vs other optimization methods - accuracy}{figure.caption.7}{}}
\newlabel{sub@fig:adam_accuracy}{{a}{6}{Adam vs other optimization methods - accuracy}{figure.caption.7}{}}
\newlabel{fig:adam_loss}{{7b}{6}{Adam vs other optimization methods - loss}{figure.caption.7}{}}
\newlabel{sub@fig:adam_loss}{{b}{6}{Adam vs other optimization methods - loss}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Optimal Hyperparameters and Comparison}{7}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{7}{section.4}\protected@file@percent }
\bibcite{kingma2014adam}{1}
\bibcite{lecun1998gradient}{2}
\bibcite{bottou2008tradeoffs}{3}
\gdef \@abspage@last{8}
